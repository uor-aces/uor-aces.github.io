
@inproceedings{OspEA13,
  timestamp = {2018-01-22T15:22:53Z},
  location = {{Las Vegas}},
  title = {A {{Benchmark}}-{{Driven Modelling Approach}} for {{Evaluating Deployment Choices}} on a {{Multicore Architecture}}},
  isbn = {1-60132-258-5},
  booktitle = {Proceedings of the {{International Conference}} on {{Parallel}} \& {{Distributed Processing Techniques}} \& {{Application}} ({{PDPTA}}'13)},
  author = {Osprey, A. and Riley, G.D. and Manjunathaiah, M. and Lawrence, B.N.},
  date = {2013-07},
  pages = {571--577},
  keywords = {bnl-cv,bnl-cv-refconf,bnl-cv-refgen,aces},
  file = {PDP3558.pdf:/Users/BNL28/zotero-storage/storage/MF9AAZ8C/PDP3558.pdf:application/pdf}
}

@inproceedings{OspEA14,
  timestamp = {2018-02-04T20:16:31Z},
  title = {The Development of a Data-Driven Application Benchmarking Approach to Performance Modelling},
  doi = {10.1109/HPCSim.2014.6903760},
  abstract = {Performance modelling is a useful tool in the lifeycle of high performance scientific software, such as weather and climate models, especially as a means of ensuring efficient use of available computing resources. In particular, sufficiently accurate performance prediction could reduce the effort and experimental computer time required when porting and optimising a climate model to a new machine. Yet as architectures become more complex, performance prediction is becoming more difficult. Traditional methods of performance prediction, based on source code analysis and supported by machine benchmarks, are proving inadequate to the task. In this paper, the reasons for this are explored by applying some traditional techniques to predict the computation time of a simple shallow water model which is illustrative of the computation (and communication) involved in climate models. These models are compared with real execution data gathered on AMD Opteron-based systems, including several phases of the U.K. academic community HPC resource, HECToR. Some success is had in relating source code to achieved performance for the K10 series of Opterons, but the method is found to be inadequate for the next-generation Interlagos processor. The experience leads to the investigation of a data-driven application benchmarking approach to performance modelling. Results for an early version of the approach are presented using the shallow model as an example. In addition, the data-driven approach is compared with a novel analytical model based on fitting logarithmic curves to benchmarked application data. The limitations of this analytical method provide further motivation for the development of the data-driven approach and results of this work have been published elsewhere.},
  eventtitle = {2014 {{International Conference}} on {{High Performance Computing Simulation}} ({{HPCS}})},
  booktitle = {2014 {{International Conference}} on {{High Performance Computing Simulation}} ({{HPCS}})},
  author = {Osprey, A. and Riley, G.D. and Manjunathaiah, M. and Lawrence, B.N.},
  date = {2014-07},
  pages = {715--723},
  keywords = {bnl-cv,Bandwidth,parallel processing,geophysics computing,Climate model,Meteorology,Computer architecture,Benchmark testing,curve fitting,source code (software),AMD Opteron-based systems,HECToR,UK academic community HPC resource,data-driven application benchmarking approach development,high performance scientific software lifeycle,logarithmic curve fitting,machine benchmarks,next-generation Interlagos processor,performance modelling,performance prediction,shallow water model,source code analysis,Analytical models,Computational modeling,Mathematical model,benchmarking,multicore,bnl-cv-refconf,bnl-cv-refgen,aces},
  file = {IEEE Xplore Abstract Record:/Users/BNL28/zotero-storage/storage/766H76DP/abstractAuthors.html:text/html}
}

@unpublished{KunEA16,
  timestamp = {2018-02-06T08:23:26Z},
  location = {{Salt Lake City}},
  title = {Middleware for {{Earth System Data}}},
  url = {http://www.pdsw.org/pdsw-discs16/wips/kunkel1-wip-pdsw-discs16.pdf},
  type = {Work in {{Progress}}},
  howpublished = {Work in Progress},
  eventtitle = {1st {{International Workshop}} on {{Parallell Data Storage}} and {{Data Intensive Scalable Computing Systems}} ({{PDSW}}-{{DISCS}}'16)},
  author = {Kunkel, Julian and Luettgau, Jakob and Lawrence, Bryan N. and Jensen, Jens and Congiu, Giuseppe and Readey, John},
  urldate = {2017-06-23},
  date = {2016},
  keywords = {bnl,prp17,bnl-cv-conf,aces},
  file = {kunkel1-wip-pdsw-discs16.pdf:/Users/BNL28/zotero-storage/storage/PE3Z3XWQ/kunkel1-wip-pdsw-discs16.pdf:application/pdf}
}

@article{BalEA17p,
  timestamp = {2018-02-12T19:12:10Z},
  title = {{{CPMIP}}: Measurements of Real Computational Performance of {{Earth}} System Models in {{CMIP6}}},
  volume = {10},
  issn = {1991-9603},
  doi = {10.5194/gmd-10-19-2017},
  shorttitle = {{{CPMIP}}},
  abstract = {A climate model represents a multitude of processes on a variety of timescales and space scales: a canonical example of multi-physics multi-scale modeling. The underlying climate system is physically characterized by sensitive dependence on initial conditions, and natural stochastic variability, so very long integrations are needed to extract signals of climate change. Algorithms generally possess weak scaling and can be I/O and/or memory-bound. Such weak-scaling, I/O, and memory-bound multi-physics codes present particular challenges to computational performance.  Traditional metrics of computational efficiency such as performance counters and scaling curves do not tell us enough about real sustained performance from climate models on different machines. They also do not provide a satisfactory basis for comparative information across models. codes present particular challenges to computational performance.  We introduce a set of metrics that can be used for the study of computational performance of climate (and Earth system) models. These measures do not require specialized software or specific hardware counters, and should be accessible to anyone. They are independent of platform and underlying parallel programming models. We show how these metrics can be used to measure actually attained performance of Earth system models on different machines, and identify the most fruitful areas of research and development for performance engineering. codes present particular challenges to computational performance.  We present results for these measures for a diverse suite of models from several modeling centers, and propose to use these measures as a basis for a CPMIP, a computational performance model intercomparison project (MIP).},
  number = {1},
  journaltitle = {Geosci. Model Dev.},
  shortjournal = {Geosci. Model Dev.},
  author = {Balaji, V. and Maisonnave, E. and Zadeh, N. and Lawrence, B. N. and Biercamp, J. and Fladrich, U. and Aloisio, G. and Benson, R. and Caubel, A. and Durachta, J. and Foujols, M.-A. and Lister, G. and Mocavero, S. and Underwood, S. and Wright, G.},
  date = {2017-01-02},
  pages = {19--34},
  keywords = {bnl,bnl-cv,chasm,prp17,bnl-cv-refgen,bnl-cv-refjnl,onblog,aces},
  file = {Geosci. Model Dev. PDF:/Users/BNL28/zotero-storage/storage/N79RNUKT/Balaji et al. - 2017 - CPMIP measurements of real computational performa.pdf:application/pdf}
}

@inproceedings{MasEA17,
  timestamp = {2018-02-13T10:55:08Z},
  location = {{Toulouse, France}},
  title = {Evolving {{JASMIN}}: {{High}} Performance Analysis and the Data Deluge},
  doi = {10.2760/383579},
  abstract = {JASMIN is a highly successful data analysis system, which is used by thousands of academics and their industrial partners to analyse many petabytes of environmental data. The rapidly increasing volume of data stored on JASMIN, and the steadily increasing number of users, is making it necessary to investigate and implement new methods of providing computing resources to the users, storing the data that they produce from their analyses and storing and maintaining a very large archive of environmental data. To achieve this, two main areas of research are described. Firstly, providing users with virtualised services to best utilise the computing resources available. Secondly, using object storage to provide a large, yet affordable, data store and providing the users with tools and interfaces to common environmental data formats, so as to not unduly affect their current work flows.},
  booktitle = {Proc. of the 2017 Conference on {{Big Data}} from {{Space}} ({{BiDS}}’17)},
  author = {Massey, Neil and Kershaw, Philip and Pritchard, Matt and Pryor, Matt and Pepler, Sam and Churchill, Jonathan and Lawrence, Bryan},
  date = {2017},
  pages = {287--288},
  keywords = {bnl,bnl-cv,bnl-cv-conf,aces},
  file = {MasEA17.pdf:/Users/BNL28/zotero-storage/storage/63GBYWD2/MasEA17.pdf:application/pdf}
}

@unpublished{LueEA17,
  timestamp = {2018-02-13T10:54:29Z},
  location = {{Denver}},
  title = {Towards {{Structure}}-{{Aware Earth System Data Management}}},
  abstract = {Current storage environments confront domain scientist and data
center operators with usability and performance challenges. To
achieve performance portability data description libraries such as
HDF5 and NetCDF are widely adopted. At the moment, these libraries
struggle to adequately account for access patterns when
reading and writing data to multi-tier distributed storage systems.
As part of the ESiWACE[1] project, we develop a novel I/O middleware
targeting, but not limited to, earth system data. The architecture
builds on top of well established end-user interfaces but utilizes
scientific metadata to harness a data structure centric perspective.},
  type = {Work in {{Progress}}},
  howpublished = {Work in Progress},
  eventtitle = {2nd {{International Workshop}} on {{Parallel Data Storage}} and {{Data Intensive Scalable Computing Systems}} ({{PDSW}}-{{DISCS}}'17)},
  author = {Luettgau, Jakob and Kunkel, Julian and Lawrence, Bryan N. and Fiore, Sandro and Hua, Huang},
  date = {2017-11-13},
  keywords = {bnl,bnl-cv,bnl-cv-conf,aces},
  file = {LueEA17.pdf:/Users/BNL28/zotero-storage/storage/YQUWLZ4C/LueEA17.pdf:application/pdf}
}

@article{HubbeReducingHPCdatastorageFootprint2013,
  timestamp = {2018-02-20T19:11:33Z},
  title = {Reducing the {{HPC}}-Datastorage {{Footprint}} with {{MAFISC}}–{{Multidimensional Adaptive Filtering Improved Scientific Data Compression}}},
  volume = {28},
  issn = {1865-2034},
  url = {http://dx.doi.org/10.1007/s00450-012-0222-4},
  doi = {10.1007/s00450-012-0222-4},
  abstract = {Large HPC installations today also include large data storage installations. Data compression can significantly reduce the amount of data, and it was one of our goals to find out, how much compression can do for climate data. The price of compression is, of course, the need for additional computational resources, so our second goal was to relate the savings of compression to the costs it necessitates.In this paper we present the results of our analysis of typical climate data. A lossless algorithm based on these insights is developed and its compression ratio is compared to that of standard compression tools. As it turns out, this algorithm is general enough to be useful for a large class of scientific data, which is the reason we speak of MAFISC as a method for scientific data compression. A numeric problem for lossless compression of scientific data is identified and a possible solution is given. Finally, we discuss the economics of data compression in HPC environments using the example of the German Climate Computing Center.},
  issue = {2-3},
  journaltitle = {Comput. Sci.},
  author = {Hübbe, Nathanael and Kunkel, Julian},
  urldate = {2018-02-20},
  date = {2013-05},
  pages = {231--239},
  keywords = {NetCDF,aces,Data compression,HDF5}
}

@article{HubbeEvaluatinglossycompression2013,
  timestamp = {2018-02-20T19:21:06Z},
  title = {Evaluating Lossy Compression on Climate Data},
  volume = {7905},
  doi = {10.1007/978-3-642-38750-0_26},
  journaltitle = {International Supercomputing Conference},
  series = {Lecture Notes in Computer Science},
  author = {Hübbe, Nathanael and Wegener, Al and Kunkel, Julian Martin and Ling, Yi and Ludwig, Thomas},
  date = {2013},
  pages = {343--356},
  keywords = {aces},
  file = {e0ef270f5cff61e2f12b2f2068172554854a.pdf:/Users/BNL28/zotero-storage/storage/MXVI9G3A/e0ef270f5cff61e2f12b2f2068172554854a.pdf:application/pdf}
}

@article{KuhnDynamicfilesystem2009,
  timestamp = {2018-02-20T19:13:11Z},
  langid = {english},
  title = {Dynamic File System Semantics to Enable Metadata Optimizations in {{PVFS}}},
  volume = {21},
  issn = {15320626, 15320634},
  url = {http://doi.wiley.com/10.1002/cpe.1439},
  doi = {10.1002/cpe.1439},
  number = {14},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  author = {Kuhn, Michael and Kunkel, Julian Martin and Ludwig, Thomas},
  urldate = {2018-02-20},
  date = {2009-09-25},
  pages = {1775--1788},
  keywords = {aces},
  file = {318b3288506266db1c02377022f4a067c4c9.pdf:/Users/BNL28/zotero-storage/storage/MXZG64KD/318b3288506266db1c02377022f4a067c4c9.pdf:application/pdf}
}

@inproceedings{Meisterstudydatadeduplication2012a,
  timestamp = {2018-02-20T19:23:41Z},
  title = {A Study on Data Deduplication in {{HPC}} Storage Systems},
  doi = {10.1109/SC.2012.14},
  abstract = {Deduplication is a storage saving technique that is highly successful in enterprise backup environments. On a file system, a single data block might be stored multiple times across different files, for example, multiple versions of a file might exist that are mostly identical. With deduplication, this data replication is localized and redundancy is removed – by storing data just once, all files that use identical regions refer to the same unique data. The most common approach splits file data into chunks and calculates a cryptographic fingerprint for each chunk. By checking if the fingerprint has already been stored, a chunk is classified as redundant or unique. Only unique chunks are stored. This paper presents the first study on the potential of data deduplication in HPC centers, which belong to the most demanding storage producers. We have quantitatively assessed this potential for capacity reduction for 4 data centers (BSC, DKRZ, RENCI, RWTH). In contrast to previous deduplication studies focusing mostly on backup data, we have analyzed over one PB (1212 TB) of online file system data. The evaluation shows that typically 20\% to 30\% of this online data can be removed by applying data deduplication techniques, peaking up to 70\% for some data sets. This reduction can only be achieved by a subfile deduplication approach, while approaches based on whole-file comparisons only lead to small capacity savings.},
  eventtitle = {High {{Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}} ({{SC}}), 2012 {{International Conference}} For},
  booktitle = {High {{Performance Computing}}, {{Networking}}, {{Storage}} and {{Analysis}} ({{SC}}), 2012 {{International Conference}} For},
  author = {Meister, D. and Kaiser, J. and Brinkmann, A. and Cortes, T. and Kuhn, M. and Kunkel, J.},
  date = {2012-11},
  pages = {1--11},
  keywords = {Educational institutions,Internet,Meteorology,Virtual machining,storage management,Indexes,aces,back-up procedures,backup data,BSC,capacity reduction,computer centres,cryptographic fingerprint calculation,cryptography,Cryptography,data block,data centers,data deduplication,DKRZ,enterprise backup environments,file data,Focusing,HPC centers,HPC storage system,localized data replication,online file system data,Redundancy,redundancy removal,RENCI,replicated databases,RWTH,storage saving technique,subfile deduplication approach},
  file = {Meister et al. - 2012 - A study on data deduplication in HPC storage syste.pdf:/Users/BNL28/zotero-storage/storage/XBUQEK35/Meister et al. - 2012 - A study on data deduplication in HPC storage syste.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/BNL28/zotero-storage/storage/ZFI96DPP/6468447.html:text/html}
}

@inproceedings{CarnsSmallfileaccessparallel2009a,
  timestamp = {2018-02-20T19:26:10Z},
  title = {Small-File Access in Parallel File Systems},
  doi = {10.1109/IPDPS.2009.5161029},
  abstract = {Today's computational science demands have resulted in ever larger parallel computers, and storage systems have grown to match these demands. Parallel file systems used in this environment are increasingly specialized to extract the highest possible performance for large I/O operations, at the expense of other potential workloads. While some applications have adapted to I/O best practices and can obtain good performance on these systems, the natural I/O patterns of many applications result in generation of many small files. These applications are not well served by current parallel file systems at very large scale. This paper describes five techniques for optimizing small-file access in parallel file systems for very large scale systems. These five techniques are all implemented in a single parallel file system (PVFS) and then systematically assessed on two test platforms. A microbenchmark and the mdtest benchmark are used to evaluate the optimizations at an unprecedented scale. We observe as much as a 905\% improvement in small-file create rates, 1,106\% improvement in small-file stat rates, and 727\% improvement in small-file removal rates, compared to a baseline PVFS configuration on a leadership computing platform using 16,384 cores.},
  eventtitle = {2009 {{IEEE International Symposium}} on {{Parallel Distributed Processing}}},
  booktitle = {2009 {{IEEE International Symposium}} on {{Parallel Distributed Processing}}},
  author = {Carns, P. and Lang, S. and Ross, R. and Vilayannur, M. and Kunkel, J. and Ludwig, T.},
  date = {2009-05},
  pages = {1--11},
  keywords = {Laboratories,Application software,Computer science,Mathematics,parallel processing,Benchmark testing,Large-scale systems,file organisation,Concurrent computing,parallel file system,aces,Best practices,File systems,large I/O operation,small-file access,small-file create rate,small-file removal rate,small-file stat rate,System testing,very large scale system},
  file = {Carns et al. - 2009 - Small-file access in parallel file systems.pdf:/Users/BNL28/zotero-storage/storage/SCHYZ23A/Carns et al. - 2009 - Small-file access in parallel file systems.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/BNL28/zotero-storage/storage/HWA2H6TW/5161029.html:text/html}
}

@article{LawrenceCrossingchasmhow2018,
  timestamp = {2018-05-08T07:52:26Z},
  langid = {english},
  title = {Crossing the Chasm: How to Develop Weather and Climate Models for next Generation Computers?},
  volume = {11},
  issn = {1991-9603},
  url = {https://www.geosci-model-dev.net/11/1799/2018/},
  doi = {10.5194/gmd-11-1799-2018},
  shorttitle = {Crossing the Chasm},
  number = {5},
  journaltitle = {Geoscientific Model Development},
  author = {Lawrence, Bryan N. and Rezny, Michael and Budich, Reinhard and Bauer, Peter and Behrens, Jörg and Carter, Mick and Deconinck, Willem and Ford, Rupert and Maynard, Christopher and Mullerworth, Steven and Osuna, Carlos and Porter, Andrew and Serradell, Kim and Valcke, Sophie and Wedi, Nils and Wilson, Simon},
  urldate = {2018-05-08},
  date = {2018-05-08},
  pages = {1799--1821},
  keywords = {bnl-cv,bnl-cv-refjnl,aces}
}


